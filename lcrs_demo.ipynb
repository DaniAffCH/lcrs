{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Conditioned Robot Skills (LCRS) - Live Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calvin_agent.utils.utils import print_system_env_info, get_last_checkpoint, format_sftp_path\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.loggers import Logger\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning import Callback, LightningModule, seed_everything, Trainer\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from pytorch_lightning.utilities.cloud_io import load as pl_load\n",
    "import hydra\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import List\n",
    "import lcrs\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@hydra.main(config_path=\"../conf\", config_name=\"config\")\n",
    "def predict(cfg: DictConfig) -> None:\n",
    "    seed_everything(cfg.seed, workers=True)\n",
    "    datamodule = hydra.utils.instantiate(cfg.datamodule, training_repo_root=Path(lcrs.__file__).parents[1])\n",
    "    model = hydra.utils.instantiate(cfg.model)\n",
    "    chk = get_last_checkpoint(Path.cwd())\n",
    "\n",
    "    if chk:\n",
    "        pretrain_chk = pl_load(format_sftp_path(Path(cfg.pretrain_chk)), map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(pretrain_chk[\"state_dict\"], strict=False)\n",
    "        \n",
    "        log_rank_0(\"LOADED PRE-TRAINED MODEL\")\n",
    "    else:\n",
    "        raise(Exception(\"No pre-trained model found\"))\n",
    "    \n",
    "    log_rank_0(print_system_env_info())\n",
    "\n",
    "\n",
    "    trainer_args = {\n",
    "        **cfg.trainer,\n",
    "        \"logger\": train_logger,\n",
    "        \"callbacks\": callbacks,\n",
    "        \"benchmark\": False,\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(**trainer_args)\n",
    "\n",
    "    predictions = trainer.predict(model, datamodule=datamodule)\n",
    "\n",
    "@rank_zero_only\n",
    "def log_rank_0(*args, **kwargs):\n",
    "    logger.info(*args, **kwargs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # train()\n",
    "    print(\"main\")\n",
    "\n",
    "\n",
    "# loaded_model = lcrs.load_from_checkpoint(\"~/runs/2024-03-07/21-32-22/saved_models/epoch=0.ckpt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup simulation environment\n",
    "\n",
    "CALVIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd calvin\n",
    "from hydra import initialize, compose\n",
    "\n",
    "with initialize(config_path=\"./calvin_env/conf/\"):\n",
    "  cfg = compose(config_name=\"config_data_collection.yaml\", overrides=[\"cameras=static_and_gripper\"])\n",
    "  cfg.env[\"use_egl\"] = False\n",
    "  cfg.env[\"show_gui\"] = False\n",
    "  cfg.env[\"use_vr\"] = False\n",
    "  cfg.env[\"use_scene_info\"] = True\n",
    "  print(cfg.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hydra\n",
    "import numpy as np\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "env = hydra.utils.instantiate(cfg.env)\n",
    "observation = env.reset()\n",
    "#The observation is given as a dictionary with different values\n",
    "print(observation.keys())\n",
    "for i in range(5):\n",
    "  # The action consists in a pose displacement (position and orientation)\n",
    "  action_displacement = np.random.uniform(low=-1, high=1, size=6)\n",
    "  # And a binary gripper action, -1 for closing and 1 for oppening\n",
    "  action_gripper = np.random.choice([-1, 1], size=1)\n",
    "  action = np.concatenate((action_displacement, action_gripper), axis=-1)\n",
    "  observation, reward, done, info = env.step(action)\n",
    "  rgb = env.render(mode=\"rgb_array\")[:,:,::-1]\n",
    "  cv2_imshow(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.step() #todo: integrate environment into it and so on\n",
    "predict\n",
    "predict_step"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
